# -*- coding: utf-8 -*-
"""MATH6380p project 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ODSfZJajROAjMUFdjI9aRbfM1rb0pHwb
"""

print('hello world')
print("hello, my name is shizhe")
print("hello, this is vincent")

print("hello, this is vincent")

"""### ResNet-18"""

import torch
import torchvision
from torch import nn
from torchvision import models, datasets, transforms

"""#### Load pretrained resnet18"""

model = models.resnet18(pretrained=True)
    
fc_in_size = model.fc.in_features
model.fc = nn.Linear(fc_in_size, 10)

"""#### Load Mnist"""

transform=transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

train_dataset = datasets.MNIST('./data', train=True, download=True,
                               transform=transform)
test_dataset = datasets.MNIST('./data', train=False,
                               transform=transform)

"""### Fine tuning"""

class AverageMeter(object):
    def __init__(self):
        self.reset()
        
    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.cnt = 0
        
    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.cnt += n
        self.avg = self.sum / self.cnt
  
def compute_acc(outputs, targets):
  preds = outputs.argmax(dim=1, keepdim=True)
  return preds.eq(targets.view_as(preds)).sum().item() / targets.shape[0]

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=1, pin_memory=True)
eval_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=False, num_workers=1, pin_memory=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=1, pin_memory=True)

from torch.optim import Adam, lr_scheduler

model = model.cuda()
optimizer = Adam(model.fc.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss().cuda()
# scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

from tqdm.auto import tqdm
from torch.autograd import Variable

# model.train()
for epoch in range(3):
    print("entering epoch ", epoch)
    losses = AverageMeter()
    accs = AverageMeter()
    for X, y in tqdm(train_loader, leave=False):
        inputs = Variable(X.cuda())
        targets = Variable(y.cuda())

        optimizer.zero_grad()

        # forward
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        # scheduler.step()
        nsample = inputs.size(0)
        accs.update(compute_acc(outputs, targets), nsample)
        losses.update(loss.item(), nsample)


    print(f'Loss: {losses.avg:.4f}\tAcc: {accs.avg:.4%}')

model.eval()
accs = AverageMeter()
losses = AverageMeter()

for X, y in tqdm(test_loader, leave=False):
  with torch.no_grad():
      inputs = Variable(X.cuda())
      targets = Variable(y.cuda())
      # forward
      outputs = model(inputs)
      nsample = inputs.size(0)
      loss = criterion(outputs, targets)
      losses.update(loss.item(), nsample)
      accs.update(compute_acc(outputs, targets), nsample)
print(f'Loss: {losses.avg:.4f}\tAcc: {accs.avg:.4%}')

from tqdm.auto import tqdm
from torch.autograd import Variable

class NoOp(nn.Module):
  def __init__(self):
    super(NoOp, self).__init__()
  
  def forward(self, x):
    return x


model.fc = NoOp()
model.eval()

features = []
labels = []
for X, y in tqdm(eval_loader):
  with torch.no_grad():
    inputs = Variable(X.cuda())
    features.append(model(inputs).cpu().numpy())
    labels.append(y.numpy())
    del inputs

for X, y in tqdm(test_loader):
  with torch.no_grad():
    inputs = Variable(X.cuda())
    features.append(model(inputs).cpu().numpy())
    labels.append(y.numpy())
    del inputs

import numpy as np
features = np.concatenate(features, axis=0)
labels = np.concatenate(labels, axis=0)
print(features.shape, labels.shape)

np.savez('./features/resnet-features.npz', features=features, labels=labels)
#
# from google.colab import drive
# drive.mount('/content/drive')
#
# import numpy as np
# npz = np.load('./drive/My Drive/resnet-features.npz')
# features = npz['features']
# labels = npz['labels']
#
# import matplotlib.pyplot as plt
#
# def visualize(z, color):
#
#     plt.figure(figsize=(10,10))
#     plt.xticks([])
#     plt.yticks([])
#
#     plt.scatter(z[:, 0], z[:, 1], s=10, c=color, cmap="Set2")
#     plt.show()
#
# from sklearn.manifold import TSNE
#
# z = TSNE(n_components=2).fit_transform(features)
# np.savez('./drive/My Drive/tsne.npz', z=z)
#
# from sklearn.decomposition import PCA
# z = PCA(n_components=2).fit_transform(features)
# np.savez('./drive/My Drive/pca.npz', z=z)
#
# visualize(z, labels)
#
# """### Calculation"""
#
# mu_G = features.mean(axis=0)
#
# mu = np.zeros((10, 512))
# for i in range(10):
#   mu[i, :] = features[labels == i, :].mean(axis=0)
#
# T = np.zeros((512, 512))
# for i in range(70000):
#   T += np.outer(features[i, :] - mu_G, features[i, :] - mu_G) / 70000
#
# B = np.zeros((512, 512))
# for i in range(10):
#   B += np.outer(mu[i, :] - mu_G, mu[i, :] - mu_G) / 10
#
# W = np.zeros((512, 512))
# for i in range(70000):
#   W += np.outer(features[i, :] - mu[labels[i], :], features[i, :] - mu[labels[i], :]) / 70000
#
# NC1 = np.trace(W @ B.T) / 10
#
# CEC = np.std(np.linalg.norm(mu - mu_G, axis=1)) / np.mean(np.linalg.norm(mu - mu_G, axis=1))
#
# cos = np.zeros((10, 10))
# for i in range(10):
#   for j in range(10):
#     cos[i, j] = np.dot(mu[i, :] - mu_G, mu[j, :] - mu_G) / np.linalg.norm(mu[i, :] - mu_G) / np.linalg.norm(mu[j, :] - mu_G)
#
# EA = np.std(cos, axis=1)
#
# CME = np.mean(np.fabs(cos + 1/(10-1)))
#
# np.savez('./drive/My Drive/computation.npz', mu_G=mu_G, mu=mu, T=T, B=B, W=W, NC1=NC1, CEC=CEC, cos=cos, EA=EA, CME=CME)
#
#
#
#

